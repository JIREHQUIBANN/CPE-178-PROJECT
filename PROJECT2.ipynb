{"cells":[{"cell_type":"code","execution_count":1,"id":"MEUUorwJZXa3","metadata":{"id":"MEUUorwJZXa3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741681171022,"user_tz":-480,"elapsed":74936,"user":{"displayName":"Abbe","userId":"04594755005455973902"}},"outputId":"3f8e7980-5465-49fe-9459-e2b0680a2b24"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mindspore\n","  Downloading mindspore-2.5.0-cp311-cp311-manylinux1_x86_64.whl.metadata (18 kB)\n","Requirement already satisfied: numpy<2.0.0,>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from mindspore) (1.26.4)\n","Requirement already satisfied: protobuf>=3.13.0 in /usr/local/lib/python3.11/dist-packages (from mindspore) (4.25.6)\n","Collecting asttokens>=2.0.4 (from mindspore)\n","  Downloading asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from mindspore) (11.1.0)\n","Requirement already satisfied: scipy>=1.5.4 in /usr/local/lib/python3.11/dist-packages (from mindspore) (1.13.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from mindspore) (24.2)\n","Requirement already satisfied: psutil>=5.6.1 in /usr/local/lib/python3.11/dist-packages (from mindspore) (5.9.5)\n","Requirement already satisfied: astunparse>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from mindspore) (1.6.3)\n","Requirement already satisfied: safetensors>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mindspore) (0.5.3)\n","Collecting dill>=0.3.7 (from mindspore)\n","  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.3->mindspore) (0.45.1)\n","Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.3->mindspore) (1.17.0)\n","Downloading mindspore-2.5.0-cp311-cp311-manylinux1_x86_64.whl (962.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m962.0/962.0 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading asttokens-3.0.0-py3-none-any.whl (26 kB)\n","Downloading dill-0.3.9-py3-none-any.whl (119 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: dill, asttokens, mindspore\n","Successfully installed asttokens-3.0.0 dill-0.3.9 mindspore-2.5.0\n"]}],"source":["pip install mindspore"]},{"cell_type":"code","execution_count":2,"id":"DlLG-Wt-NDJZ","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DlLG-Wt-NDJZ","executionInfo":{"status":"ok","timestamp":1741681192280,"user_tz":-480,"elapsed":21252,"user":{"displayName":"Abbe","userId":"04594755005455973902"}},"outputId":"59112c04-dde6-4584-c8e1-40c96789f6d8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"id":"eQX52aLW-AWA","metadata":{"id":"eQX52aLW-AWA","executionInfo":{"status":"ok","timestamp":1741681296255,"user_tz":-480,"elapsed":2959,"user":{"displayName":"Abbe","userId":"04594755005455973902"}}},"outputs":[],"source":["# Import necessary libraries\n","import os\n","import pandas as pd\n","import cv2\n","import numpy as np\n","import mindspore.nn as nn\n","from mindspore import Model, Tensor, context, load_checkpoint, load_param_into_net\n","from mindspore.train.callback import LossMonitor, ModelCheckpoint, CheckpointConfig\n","from mindspore.dataset import NumpySlicesDataset\n","from mindspore.dataset.vision import Inter, Resize, Normalize, HWC2CHW\n","from mindspore.common.initializer import Normal"]},{"cell_type":"code","execution_count":4,"id":"edztTT2p9sFB","metadata":{"id":"edztTT2p9sFB","executionInfo":{"status":"ok","timestamp":1741681296301,"user_tz":-480,"elapsed":43,"user":{"displayName":"Abbe","userId":"04594755005455973902"}}},"outputs":[],"source":["# Define paths\n","DATA_PATH = os.path.join(\"/content/drive/MyDrive/Colab Notebooks/PLD_3_Classes_256/Training\")  # Path to the DATA folder\n","TEST_PATH = os.path.join(\"/content/drive/MyDrive/Colab Notebooks/PLD_3_Classes_256/Testing\")  # Path to the TEST folder\n","LABELS_CSV_PATH = os.path.join(\"/content/drive/MyDrive/Colab Notebooks/PLD_3_Classes_256/labelsplant.csv\")  # Path to labels.csv"]},{"cell_type":"code","execution_count":null,"id":"BoIraOXo9t84","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BoIraOXo9t84","outputId":"96234d09-10a5-4b68-9681-bad02b156b98"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading training data...\n","Loading images from /content/drive/MyDrive/Colab Notebooks/PLD_3_Classes_256/Training...\n","Loading images from class 0...\n","Loading images from class 1...\n","Loading images from class 2...\n"]}],"source":["# Load and preprocess the dataset\n","def load_images_from_folder(folder, target_size=(224, 224)):\n","    print(f\"Loading images from {folder}...\")\n","    images = []\n","    labels = []\n","    for class_id in os.listdir(folder):\n","        class_folder = os.path.join(folder, class_id)\n","        if not os.path.isdir(class_folder):\n","            continue\n","        print(f\"Loading images from class {class_id}...\")\n","        for image_name in os.listdir(class_folder):\n","            image_path = os.path.join(class_folder, image_name)\n","            image = cv2.imread(image_path)\n","            if image is None:\n","                print(f\"Warning: Unable to load image {image_path}. Skipping.\")\n","                continue\n","            image = cv2.resize(image, target_size)\n","            image = image / 255.0  # Normalize to [0, 1]\n","            images.append(image)\n","            labels.append(int(class_id))\n","    print(f\"Loaded {len(images)} images from {folder}.\")\n","    return np.array(images), np.array(labels)\n","\n","# Load training and test data\n","print(\"Loading training data...\")\n","train_images, train_labels = load_images_from_folder(DATA_PATH)\n","print(\"Loading test data...\")\n","test_images, test_labels = load_images_from_folder(TEST_PATH)"]},{"cell_type":"code","execution_count":null,"id":"-Nxrb920k-H3","metadata":{"id":"-Nxrb920k-H3"},"outputs":[],"source":["# Split training data into training and validation sets\n","from sklearn.model_selection import train_test_split\n","\n","train_images, val_images, train_labels, val_labels = train_test_split(\n","    train_images, train_labels, test_size=0.2, random_state=42\n",")\n","print(f\"Training set size: {len(train_images)}\")\n","print(f\"Validation set size: {len(val_images)}\")"]},{"cell_type":"code","execution_count":null,"id":"u4_YiCRZLcq6","metadata":{"id":"u4_YiCRZLcq6"},"outputs":[],"source":["# Load labels.csv\n","print(\"Loading labels.csv...\")\n","labels_df = pd.read_csv(LABELS_CSV_PATH)\n","classid_to_name = dict(zip(labels_df[\"ClassId\"], labels_df[\"Name\"]))\n","print(\"Labels loaded successfully.\")"]},{"cell_type":"code","execution_count":null,"id":"9cZET6nJLfc6","metadata":{"id":"9cZET6nJLfc6"},"outputs":[],"source":["# Convert data to MindSpore dataset\n","def preprocess_dataset(images, labels, is_training=True):\n","    print(\"Preprocessing dataset...\")\n","    dataset = NumpySlicesDataset((images, labels), column_names=[\"image\", \"label\"], shuffle=True)\n","\n","    # Define transformations\n","    resize_op = Resize((224, 224), interpolation=Inter.BILINEAR)\n","    normalize_op = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    hwc2chw_op = HWC2CHW()\n","\n","    # Apply transformations to the \"image\" column\n","    dataset = dataset.map(operations=[resize_op, normalize_op, hwc2chw_op], input_columns=\"image\")\n","    if is_training:\n","        dataset = dataset.batch(32, drop_remainder=True)\n","    else:\n","        dataset = dataset.batch(32)\n","    print(\"Dataset preprocessing complete.\")\n","    return dataset\n","\n","print(\"Creating training dataset...\")\n","train_dataset = preprocess_dataset(train_images, train_labels, is_training=True)\n","\n","print(\"Creating validation dataset...\")\n","val_dataset = preprocess_dataset(val_images, val_labels, is_training=False)\n","\n","print(\"Creating test dataset...\")\n","test_dataset = preprocess_dataset(test_images, test_labels, is_training=False)"]},{"cell_type":"code","execution_count":null,"id":"-z12OHNEZjJf","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":402,"status":"ok","timestamp":1741265434544,"user":{"displayName":"Abbe","userId":"04594755005455973902"},"user_tz":-480},"id":"-z12OHNEZjJf","outputId":"eeda33af-866c-416a-a2b8-4964bcf741ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["Defining MobileNetV2...\n","MobileNetV2 defined successfully.\n"]}],"source":["# Define MobileNetV2 manually\n","class InvertedResidual(nn.Cell):\n","    def __init__(self, in_channels, out_channels, stride, expand_ratio):\n","        super(InvertedResidual, self).__init__()\n","        hidden_dim = in_channels * expand_ratio\n","        self.use_res_connect = stride == 1 and in_channels == out_channels\n","\n","        layers = []\n","        if expand_ratio != 1:\n","            layers.append(\n","                nn.Conv2d(in_channels, hidden_dim, kernel_size=1, stride=1, pad_mode='same', has_bias=False)\n","            )\n","            layers.append(nn.BatchNorm2d(hidden_dim))\n","            layers.append(nn.ReLU6())\n","\n","        layers.extend([\n","            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=stride, pad_mode='same', group=hidden_dim, has_bias=False),\n","            nn.BatchNorm2d(hidden_dim),\n","            nn.ReLU6(),\n","            nn.Conv2d(hidden_dim, out_channels, kernel_size=1, stride=1, pad_mode='same', has_bias=False),\n","            nn.BatchNorm2d(out_channels),\n","        ])\n","\n","        self.layers = nn.SequentialCell(layers)\n","\n","    def construct(self, x):\n","        if self.use_res_connect:\n","            return x + self.layers(x)\n","        return self.layers(x)\n","\n","class MobileNetV2(nn.Cell):\n","    def __init__(self, num_classes=58, width_mult=1.0):\n","        super(MobileNetV2, self).__init__()\n","        input_channel = 32\n","        last_channel = 1280\n","        inverted_residual_setting = [\n","            # t, c, n, s\n","            [1, 16, 1, 1],\n","            [6, 24, 2, 2],\n","            [6, 32, 3, 2],\n","            [6, 64, 4, 2],\n","            [6, 96, 3, 1],\n","            [6, 160, 3, 2],\n","            [6, 320, 1, 1],\n","        ]\n","\n","        input_channel = int(input_channel * width_mult)\n","        self.last_channel = int(last_channel * max(1.0, width_mult))\n","        features = [\n","            nn.Conv2d(3, input_channel, kernel_size=3, stride=2, pad_mode='same', has_bias=False),\n","            nn.BatchNorm2d(input_channel),\n","            nn.ReLU6(),\n","        ]\n","\n","        for t, c, n, s in inverted_residual_setting:\n","            output_channel = int(c * width_mult)\n","            for i in range(n):\n","                stride = s if i == 0 else 1\n","                features.append(InvertedResidual(input_channel, output_channel, stride, t))\n","                input_channel = output_channel\n","\n","        features.append(\n","            nn.Conv2d(input_channel, self.last_channel, kernel_size=1, stride=1, pad_mode='same', has_bias=False)\n","        )\n","        features.append(nn.BatchNorm2d(self.last_channel))\n","        features.append(nn.ReLU6())\n","\n","        self.features = nn.SequentialCell(features)\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.classifier = nn.Dense(self.last_channel, num_classes, weight_init=Normal(0.01))\n","\n","    def construct(self, x):\n","        x = self.features(x)\n","        x = self.avgpool(x)\n","        x = x.view(x.shape[0], -1)\n","        x = self.classifier(x)\n","        return x\n","\n","print(\"Defining MobileNetV2...\")\n","net = MobileNetV2(num_classes=58)  # 58 traffic sign classes\n","print(\"MobileNetV2 defined successfully.\")"]},{"cell_type":"code","execution_count":null,"id":"v-DPcnOGZoX2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":228,"status":"ok","timestamp":1741265437405,"user":{"displayName":"Abbe","userId":"04594755005455973902"},"user_tz":-480},"id":"v-DPcnOGZoX2","outputId":"1a56fd1b-8b90-43f9-f3e1-4b1c6b1e85c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Defining loss function and optimizer...\n","Loss function and optimizer defined.\n"]}],"source":["# Define loss function and optimizer\n","print(\"Defining loss function and optimizer...\")\n","loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n","optimizer = nn.Adam(params=net.trainable_params(), learning_rate=0.001)\n","print(\"Loss function and optimizer defined.\")"]},{"cell_type":"code","execution_count":null,"id":"cfQbP6dDZrIm","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1741265438575,"user":{"displayName":"Abbe","userId":"04594755005455973902"},"user_tz":-480},"id":"cfQbP6dDZrIm","outputId":"b3326e0b-62da-4518-891b-6c91e73796be"},"outputs":[{"output_type":"stream","name":"stdout","text":["Creating the Model object...\n","Model object created successfully.\n"]}],"source":["# Create the Model object\n","print(\"Creating the Model object...\")\n","model = Model(net, loss_fn=loss_fn, optimizer=optimizer, metrics={'accuracy', 'loss'})\n","print(\"Model object created successfully.\")"]},{"cell_type":"code","execution_count":null,"id":"hlcJa9g-a4q6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hlcJa9g-a4q6","outputId":"69572f6b-a564-48dc-e6a1-1351f4606bd0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training the model...\n","Recreating training dataset...\n","Preprocessing dataset...\n","Dataset preprocessing complete.\n","Steps per epoch: 81\n","Epoch: [1], Step: [1], Training Loss: [4.0497527], Validation Loss: [4.037478583199637]\n","Epoch: [1], Step: [2], Training Loss: [3.0896761], Validation Loss: [3.9975154854002453]\n","Epoch: [1], Step: [3], Training Loss: [2.3341508], Validation Loss: [3.921582573936099]\n","Epoch: [1], Step: [4], Training Loss: [1.6465596], Validation Loss: [3.7921697412218367]\n"]}],"source":["# Train the model with checkpoint saving\n","print(\"Training the model...\")\n","\n","#Load the checkpoint (Add after restart)\n","#checkpoint_path = \"/content/drive/MyDrive/CPE178P/DOE/checkpoints_steps/potato_leaf_model_steps_5-1_20.ckpt\"\n","#param_dict = load_checkpoint(checkpoint_path)\n","#load_param_into_net(net, param_dict)\n","\n","# Recreate the training dataset (Remove after recreation)\n","print(\"Recreating training dataset...\")\n","train_dataset = preprocess_dataset(train_images, train_labels, is_training=True)\n","\n","# Calculate steps per epoch\n","steps_per_epoch = len(train_dataset)\n","print(f\"Steps per epoch: {steps_per_epoch}\")\n","\n","# Define checkpoint configuration for saving every 10 steps\n","config_ck_steps = CheckpointConfig(\n","    save_checkpoint_steps=10,  # Save checkpoint every 10 steps\n","    keep_checkpoint_max=10,    # Keep only the latest 10 checkpoints\n",")\n","\n","# Define checkpoint callback for saving every 10 steps\n","ckpt_callback_steps = ModelCheckpoint(\n","    prefix=\"potato_leaf_model_steps\",  # Prefix for checkpoint filenames\n","    directory=\"/content/drive/MyDrive/CPE178P/DOE/checkpoints_steps\",    # Directory to save checkpoints\n","    config=config_ck_steps,\n",")\n","\n","# Define checkpoint configuration for saving every epoch\n","config_ck_epochs = CheckpointConfig(\n","    save_checkpoint_steps=steps_per_epoch,  # Save checkpoint every epoch\n","    keep_checkpoint_max=10,                # Keep only the latest 10 checkpoints\n",")\n","\n","# Define checkpoint callback for saving every epoch\n","ckpt_callback_epochs = ModelCheckpoint(\n","    prefix=\"potato_leaf_model_epochs\",  # Prefix for checkpoint filenames\n","    directory=\"/content/drive/MyDrive/CPE178P/DOE/checkpoints_epochs\",    # Directory to save checkpoints\n","    config=config_ck_epochs,\n",")\n","\n","# Custom callback to print epoch, step, training loss, and validation loss\n","class EpochStepLossMonitor(LossMonitor):\n","    def __init__(self, val_dataset):\n","        super(EpochStepLossMonitor, self).__init__()\n","        self.val_dataset = val_dataset\n","\n","    def on_train_step_end(self, run_context):\n","        cb_params = run_context.original_args()\n","        epoch = cb_params.cur_epoch_num\n","        step = cb_params.cur_step_num\n","        train_loss = cb_params.net_outputs\n","\n","        # Calculate validation loss\n","        val_loss = model.eval(self.val_dataset, dataset_sink_mode=False)['loss']\n","        print(f\"Epoch: [{epoch}], Step: [{step}], Training Loss: [{train_loss}], Validation Loss: [{val_loss}]\")\n","\n","# Train the model with checkpoint callbacks\n","model.train(epoch=10, train_dataset=train_dataset, callbacks=[EpochStepLossMonitor(val_dataset), ckpt_callback_steps, ckpt_callback_epochs])\n","print(\"Training complete.\")\n","\n","# Custom callback to print epoch, step, and loss\n","class EpochStepLossMonitor(LossMonitor):\n","    def on_train_step_end(self, run_context):\n","        cb_params = run_context.original_args()\n","        epoch = cb_params.cur_epoch_num\n","        step = cb_params.cur_step_num\n","        loss = cb_params.net_outputs\n","        print(f\"Epoch: {epoch}, Step: {step}, Loss is {loss}\")"]},{"cell_type":"code","execution_count":null,"id":"5R_ERBfHZtce","metadata":{"id":"5R_ERBfHZtce","colab":{"base_uri":"https://localhost:8080/","height":564},"executionInfo":{"status":"error","timestamp":1741150537072,"user_tz":-480,"elapsed":39,"user":{"displayName":"Abbe","userId":"04594755005455973902"}},"outputId":"e6754641-49c1-499d-e886-5d2796ee6ac3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluating the model...\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"Syntax error. \n\n------------------------------------------------------------------\n- Dataset Pipeline Error Message: \n------------------------------------------------------------------\n[ERROR] GeneratorNode: data row of input source must not be 0, got: 0.\n\n------------------------------------------------------------------\n- C++ Call Stack: (For framework developers) \n------------------------------------------------------------------\nmindspore/ccsrc/minddata/dataset/engine/ir/datasetops/source/generator_node.cc(135).\n\n\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-6285c78569bd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluating the model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test Accuracy: {result['accuracy']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mindspore/train/model.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, valid_dataset, callbacks, dataset_sink_mode)\u001b[0m\n\u001b[1;32m   1779\u001b[0m         \u001b[0mcb_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eval_network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m         \u001b[0mcb_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1781\u001b[0;31m         \u001b[0mcb_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1782\u001b[0m         \u001b[0mcb_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"eval\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m         \u001b[0mcb_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_step_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mindspore/dataset/engine/iterators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0moriginal_iterators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mITERATORS_LIST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# it is used to attribute function like: dataset_size / output_shapes / output_types and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mindspore/dataset/engine/datasets.py\u001b[0m in \u001b[0;36mget_dataset_size\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1876\u001b[0m         \"\"\"\n\u001b[1;32m   1877\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1878\u001b[0;31m             \u001b[0mruntime_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init_size_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1879\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mruntime_getter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetDatasetSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1880\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_size\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mindspore/dataset/engine/datasets.py\u001b[0m in \u001b[0;36m__init_size_getter\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0mGet\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0minformation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1736\u001b[0m         \"\"\"\n\u001b[0;32m-> 1737\u001b[0;31m         \u001b[0mir_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_ir_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1739\u001b[0m         \u001b[0mruntime_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcde\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRuntimeContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mindspore/dataset/engine/datasets.py\u001b[0m in \u001b[0;36mcreate_ir_tree\u001b[0;34m(self, getter_mode)\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0m_OP_NAME\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0m_OP_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_operator_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mir_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0m_init_device_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mindspore/dataset/engine/datasets.py\u001b[0m in \u001b[0;36mparse_tree\u001b[0;34m(self, getter_mode)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The data pipeline is not a tree (i.e., one node has 2 consumers)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mir_children\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m         \u001b[0;31m# Bootstrap can only be performed on a copy of the original dataset node.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0;31m# Bootstrap on original dataset node will make all iterators share the same process pool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mindspore/dataset/engine/datasets.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The data pipeline is not a tree (i.e., one node has 2 consumers)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mir_children\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m         \u001b[0;31m# Bootstrap can only be performed on a copy of the original dataset node.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0;31m# Bootstrap on original dataset node will make all iterators share the same process pool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mindspore/dataset/engine/datasets.py\u001b[0m in \u001b[0;36mparse_tree\u001b[0;34m(self, getter_mode)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The data pipeline is not a tree (i.e., one node has 2 consumers)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mir_children\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m         \u001b[0;31m# Bootstrap can only be performed on a copy of the original dataset node.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0;31m# Bootstrap on original dataset node will make all iterators share the same process pool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mindspore/dataset/engine/datasets.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The data pipeline is not a tree (i.e., one node has 2 consumers)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mir_children\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m         \u001b[0;31m# Bootstrap can only be performed on a copy of the original dataset node.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0;31m# Bootstrap on original dataset node will make all iterators share the same process pool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mindspore/dataset/engine/datasets.py\u001b[0m in \u001b[0;36mparse_tree\u001b[0;34m(self, getter_mode)\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator_bootstrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         \u001b[0mir_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mir_children\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m         \u001b[0mir_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mir_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mir_node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mindspore/dataset/engine/datasets_user_defined.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, children)\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_multiprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1018\u001b[0;31m             return cde.GeneratorNode(self.prepared_source, self.column_names, self.column_types, self.source_len,\n\u001b[0m\u001b[1;32m   1019\u001b[0m                                      self.sampler, self.num_parallel_workers, self.sample_fn, self.has_batch_sampler)\n\u001b[1;32m   1020\u001b[0m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Syntax error. \n\n------------------------------------------------------------------\n- Dataset Pipeline Error Message: \n------------------------------------------------------------------\n[ERROR] GeneratorNode: data row of input source must not be 0, got: 0.\n\n------------------------------------------------------------------\n- C++ Call Stack: (For framework developers) \n------------------------------------------------------------------\nmindspore/ccsrc/minddata/dataset/engine/ir/datasetops/source/generator_node.cc(135).\n\n\n"]}],"source":["# Evaluate the model\n","print(\"Evaluating the model...\")\n","result = model.eval(test_dataset)\n","print(f\"Test Accuracy: {result['accuracy']}\")"]},{"cell_type":"code","execution_count":null,"id":"xwIhDWn1ZzyO","metadata":{"id":"xwIhDWn1ZzyO"},"outputs":[],"source":["# Save the final model\n","print(\"Saving the final model...\")\n","from mindspore import save_checkpoint\n","save_checkpoint(net, \"traffic_sign_model_final.ckpt\")\n","print(\"Final model saved as traffic_sign_model_final.ckpt.\")"]},{"cell_type":"code","execution_count":null,"id":"YhoHi9H1Z1fu","metadata":{"id":"YhoHi9H1Z1fu"},"outputs":[],"source":["# Inference\n","def predict(image_path, model):\n","    print(f\"Predicting image {image_path}...\")\n","    image = cv2.imread(image_path)\n","    if image is None:\n","        print(f\"Error: Unable to load image {image_path}.\")\n","        return None\n","    image = cv2.resize(image, (224, 224))\n","    image = image / 255.0  # Normalize\n","    image = Tensor(image, mstype.float32)\n","    output = model.predict(image)\n","    predicted_class = np.argmax(output.asnumpy(), axis=1)\n","    predicted_class_id = predicted_class[0]\n","    predicted_class_name = classid_to_name.get(predicted_class_id, \"Unknown\")\n","    print(f\"Predicted class ID: {predicted_class_id}, Name: {predicted_class_name}\")\n","    return predicted_class_id, predicted_class_name"]},{"cell_type":"code","execution_count":null,"id":"JiOnUnatZ4le","metadata":{"id":"JiOnUnatZ4le"},"outputs":[],"source":["# Example usage\n","print(\"Running inference on a test image...\")\n","test_image_path = os.path.join(TEST_PATH, \"test_image.jpg\")\n","predicted_class_id, predicted_class_name = predict(test_image_path, model)\n","print(f\"Predicted Class ID: {predicted_class_id}, Name: {predicted_class_name}\")"]},{"cell_type":"code","execution_count":null,"id":"6L5YDeS3k7Yy","metadata":{"id":"6L5YDeS3k7Yy"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.21"}},"nbformat":4,"nbformat_minor":5}